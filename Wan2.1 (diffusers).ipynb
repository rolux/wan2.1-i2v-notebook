{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNE4rE2D3BEaDzrfPGj2sUl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["##Mount drive"],"metadata":{"id":"Zqk-g49iyHGR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPTTYncRx7lb"},"outputs":[],"source":["# pick some other OUTPUT_DIR if you don't trust this notebook to write to your drive\n","\n","import os\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","\n","INPUT_DIR = \"/content/drive/MyDrive/Wan2.1/inputs\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Wan2.1/outputs\"\n","os.makedirs(INPUT_DIR, exist_ok=True)\n","os.makedirs(OUTPUT_DIR, exist_ok=True)"]},{"cell_type":"markdown","source":["##Install libraries"],"metadata":{"id":"QD6PFpQEyJOZ"}},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/diffusers.git\n","!pip install -U bitsandbytes\n","!pip install ftfy"],"metadata":{"id":"Lc9io9ZEyKqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Assemble pipeline"],"metadata":{"id":"wCp5_F3_zMg5"}},{"cell_type":"code","source":["# for optimizations, see: https://huggingface.co/blog/video_gen\n","\n","import torch\n","from diffusers import (\n","    BitsAndBytesConfig,\n","    WanImageToVideoPipeline,\n","    WanTransformer3DModel\n",")\n","from diffusers.hooks import apply_layerwise_casting\n","from diffusers.utils import export_to_video\n","from transformers import AutoTokenizer, T5EncoderModel\n","\n","model_id = \"Wan-AI/Wan2.1-I2V-14B-480P-Diffusers\"\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","transformer = WanTransformer3DModel.from_pretrained(\n","    model_id,\n","    subfolder=\"transformer\",\n","    quantization_config=quantization_config,\n",")\n","pipe = WanImageToVideoPipeline.from_pretrained(\n","    model_id,\n","    transformer=transformer,\n",")\n","pipe.enable_model_cpu_offload()"],"metadata":{"id":"3Zd9eD30zPIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Define functions"],"metadata":{"id":"aIBw4ICH6kR9"}},{"cell_type":"code","source":["# for details, see https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/wan/pipeline_wan_i2v.py\n","\n","from PIL import Image\n","\n","def render(\n","    filename,\n","    prompt,\n","    seed=0,\n","    image=None,\n","    width=832,\n","    height=480,\n","    num_frames=81,\n","    num_inference_steps=30,\n","    guidance_scale=5.0,\n","    fps=16,\n","    extract_frames=False\n","):\n","\n","    if os.path.exists(filename):\n","        return\n","    print(filename)\n","\n","    ratio = width / height\n","    image_width, image_height = image.size\n","    image_ratio = image_width / image_height\n","    if image_ratio > ratio:\n","        w = int(image_height * ratio)\n","        c = (image_width - w) // 2\n","        image.crop((c, 0, image_width - c, image_height))\n","    elif image_ratio < ratio:\n","        h = int(image_width / ratio)\n","        c = (image_height - h) // 2\n","        image.crop((0, c, image_width, image_height - c))\n","    image.resize((width, height), Image.LANCZOS)\n","\n","    video = pipe(\n","        image=image,\n","        prompt=prompt,\n","        generator=torch.Generator(device=pipe.device).manual_seed(seed),\n","        width=width,\n","        height=height,\n","        num_frames=num_frames,\n","        num_inference_steps=num_inference_steps,\n","        guidance_scale=guidance_scale,\n","    ).frames[0]\n","    os.makedirs(os.path.dirname(filename), exist_ok=True)\n","    export_to_video(video, filename, fps=fps)\n","\n","    if extract_frames:\n","        dirname = filename[:-4]\n","        os.makedirs(dirname, exist_ok=True)\n","        for i, frame in enumerate(video):\n","            frame.save(f\"{dirname}/{i:08d}.png\")\n"],"metadata":{"id":"GME0khwh6ltw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Test"],"metadata":{"id":"a0dcUqeu9nEG"}},{"cell_type":"code","source":["# bring your own images and prompts\n","\n","src_dirname = \"/content/drive/MyDrive/FLUX.1/outputs/dev/test/random_words_1280x720\"\n","ids = sorted(\n","    [int(f.split(\",\")[-1][:-4]) for f in os.listdir(src_dirname) if f.endswith(\".txt\")]\n",")\n","prompts = {\n","    id: [f for f in os.listdir(src_dirname) if f.endswith(f\",{id}.txt\")][0]\n","    for id in ids\n","}\n","images = {\n","    id: [f for f in os.listdir(src_dirname) if f.endswith(f\",{id}.png\")][0]\n","    for id in ids\n","}\n","num_frames = 81\n","n = 3\n","for id in ids:\n","    for i in range(n):\n","        if i == 0:\n","            image = Image.open(f\"{src_dirname}/{images[id]}\")\n","        else:\n","            image = Image.open(f\"{OUTPUT_DIR}/random_words/{id},{i - 1}/{num_frames - 1:08d}.png\")\n","        render(\n","            filename=f\"{OUTPUT_DIR}/random_words/{id},{i}.mp4\",\n","            prompt=open(f\"{src_dirname}/{prompts[id]}\").read(),\n","            seed=id + i,\n","            image=image,\n","            num_frames=num_frames,\n","            extract_frames=True\n","        )\n","\n","# this will use ~21 GB of VRAM and take ~150 seconds per step\n","# (with 30 steps, that's ~75 minutes for 5 seconds of video)\n"],"metadata":{"id":"CQDNTGbd9mgu"},"execution_count":null,"outputs":[]}]}